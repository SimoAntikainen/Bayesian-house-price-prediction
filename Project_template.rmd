---
title: "Project template"
author: "Anonymous"
output: 
  pdf_document: 
    toc: yes
    toc_depth: 1
---

# 1. Introduction

(motivate the problem and )


https://www.kaggle.com/harlfoxem/housesalesprediction



# 2. Dataset description

```{r}
library(rstan)
library(shinystan)
options(mc.cores = 1)#parallel::detectCores())
library(ggplot2)
library(matrixStats)
library(dplyr)
library(GGally)
library(corrplot)
library(reshape2)
library(loo)
library(bayesplot)
```



```{r}
houseprice = read.csv("data/kc_house_data.csv", header = TRUE)
houseprice$log_price = log(houseprice$price)
datecol <- as.POSIXct(houseprice$date, format="%Y%m%dT%H%M%S")
houseprice$date_num = as.numeric(datecol)
unique_zips = unique(houseprice$zipcode)
houseprice$mutated_zipcode = match(houseprice$zipcode,  unique_zips)
head(houseprice)
```
```{r}
#https://stats.stackexchange.com/questions/8071/how-to-choose-between-pearson-and-spearman-correlation
#houseprice = houseprice[,3:21]
M <- cor(houseprice[-2], method="pearson")
corrplot(M, method = "circle")
```




```{r}
M <- cor(houseprice[-2], method="spearman")
corrplot(M, method = "circle")
```

```{r}
# wow this slow
M <- cor(houseprice[3:8], method="kendall")
corrplot(M, method = "circle")
```


```{r}
#ggpairs(houseprice  %>% select(price, bathrooms, sqft_living))
#houseprice_zipped <- houseprice
#houseprice_zipped$zipcode <- as.factor(houseprice_zipped$zipcode)
#ggpairs(houseprice_zipped, columns = c(1,2,3,4), ggplot2::aes(colour=zipcode)) 
ggpairs(houseprice  %>% select(price, bathrooms, sqft_living, grade, sqft_above, sqft_living15))
#ggpairs(houseprice, columns = c("log_price", "bathrooms"), ggplot2::aes(colour=factor(as.integer(lat*20)), alpha=0.75)) 
```

# 3. Model description


## 3.1 Prior choices

(these are good priors because referece to something)


```{r}
hist(houseprice$price)
hist(houseprice$log_price)
hist(houseprice$bathrooms)
hist(houseprice$sqft_living)
hist(houseprice$grade)
hist(houseprice$sqft_living15)
```

## 3.2 Stan models

```{r}

cat(readLines('models/multiple_linear.stan'), sep='\n')

```
```{r}
# scale the columns to 0 mean and 1 sd
houseprice_scaled <- mutate_if(houseprice, is.numeric, list(~scale(.) %>% as.vector))
#colMeans(select_if(houseprice_scaled, is.numeric)) # the means are not exactly 0 but close
#sapply(select_if(houseprice_scaled, is.numeric), sd)
#head(houseprice_scaled)
usable_numeric_columns = c("date_num", "lat", "long", "yr_built", "sqft_basement", "sqft_above", "grade", "condition", "view", "waterfront", "floors", "sqft_lot", "sqft_living", "bathrooms", "bedrooms")
```

```{r}
training_indices = 0:1000
testing_indices = 1001:1200

used_columns = usable_numeric_columns
target_column = c("log_price")
group_column =  c("mutated_zipcode")
original_target = houseprice[,target_column]
training_data = houseprice_scaled[training_indices,used_columns]
testing_data =  houseprice_scaled[testing_indices, used_columns]
training_target = houseprice_scaled[training_indices,target_column]
testing_target_scaled =  houseprice_scaled[testing_indices, target_column]
testing_target =  houseprice[testing_indices, target_column]

X_var = training_data
X_var_pred = testing_data
y_var = training_target
group_var = houseprice[training_indices,group_column]
group_var_pred = houseprice[testing_indices,group_column]
  
data_list = list(
  X = X_var,
  X_pred = X_var_pred,
  K = ncol(X_var),
  N = nrow(X_var),
  N_pred = nrow(X_var_pred),
  N_groups = length(unique_zips),
  y = y_var,
  groups = group_var,
  groups_pred  = group_var_pred
)
head(X_var)
```


```{r}
multiple_linear_fit <- stan(file = 'models/grouped_multiple_linear.stan', data = data_list, max_treedepth=12)
```

```{r}
print(multiple_linear_fit)
```
```{r}
denormalize_results <- function(new_values, sd, mean){
  return (new_values * sd + mean)
}
```

```{r}
predicted_draws = extract(multiple_linear_fit)$y_pred
predicted_raws = colQuantiles(predicted_draws, probs = c(0.05, 0.5, 0.95))
predicted_prices = denormalize_results(predicted_raws, sd(original_target), mean(original_target))
```
```{r}
result_testing = data.frame(price = houseprice[testing_indices, "price"], predicted = exp(predicted_prices))
ggpairs(result_testing, columns = c("price", "predicted.50."))#, #ggplot2::aes(colour=factor(as.integer(lat*20)), alpha=0.75)) 
```
 
```{r}
violin_predicted = extract(multiple_linear_fit)$y_pred
violin_predicted = exp(denormalize_results(violin_predicted, sd(original_target), mean(original_target)))
#violin_predicted = exp(predicted_prices)
violin_groups = outer(1:nrow(violin_predicted), 1:ncol(violin_predicted), FUN=function(r,c) unique_zips[group_var_pred[c]] )
violin_predicted = c(t(violin_predicted))
violin_groups = as.factor(c(t(violin_groups)))
violin_data_list_thing = data.frame(price=violin_predicted, group=violin_groups)

p <- ggplot(violin_data_list_thing, aes(x=group, y=price)) + 
  geom_violin()
p
#head(violin_predicted$y_pred)
```
## 3.3 Running the model

(amount of chains etc and the final fit)


# 4. Convergences diagnostics









# 5. Posterior predictive checking

```{r}
replicated_data = denormalize_results(extract(multiple_linear_fit)$y_rep, sd(original_target), mean(original_target))
#launch_shinystan(multiple_linear_fit)
p1 = hist(replicated_data, breaks = 50)
p2 = hist(original_target, breaks = 50)
#plot( p1, col=rgb(0,0,1,1/4), xlim=c(10,16))  # first histogram
#plot( p2, col=rgb(1,0,0,1/4), xlim=c(10,16), add=T)  # second

hist(exp(replicated_data), breaks = 50)
hist(exp(original_target), breaks = 100)
#yrep_lin <- posterior_predict(multiple_linear_fit)
#ppc_loo_pit_overlay(roaches$y, yrep_lin, lw = weights(loo_lin$psis_object))
```
```{r}

loo_lin <- loo(multiple_linear_fit, save_psis = TRUE)
psis_lin <- loo_lin$psis_object
lw_lin <- weights(psis_lin)
pp_check(c(original_target[training_indices]), yrep = replicated_data, fun = "stat")
ppc_loo_pit_overlay(c(original_target[training_indices]), yrep = replicated_data, lw = lw_lin)
ppc_loo_ribbon(c(original_target[training_indices]), yrep = replicated_data, lw = lw_lin, psis_object = psis_lin)
ppc_loo_intervals(c(original_target[training_indices]), yrep = replicated_data, psis_object = psis_lin)
```


# 6. Model comparison

Using leave one out (LOO), the linear model has 5 pareto k values between 0.7 and 1, and 11 values between 0.5 and 0.7. The loo values can therefore be slightly biased

```{r}
# Extract pointwise log-likelihood and compute LOO
loo_lin <- loo(multiple_linear_fit, save_psis = TRUE)
plot(loo_lin, diagnostic = c("k", "n_eff"),
  label_points = FALSE, main = "PSIS diagnostic plot for linear model")
print(loo_lin)
```

```{r}
#print(compare(x = list(loo_lin, loo_non_lin)))
```

# 7. Predictive performance assesment


# 8. Sensitivity analysis



# 9. Discussion

Wrt using separate beta for each group:
There are 70 groups, so using a different beta value for each parameter for each group would increase the number of parameters of the model by 2-17 times, likely slowing the model. In addition, the number of data usable for each beta value would shrink. The dataset is large, so using most of the dataset for training, it shouldn't be a problem but with less data it could lead to overfitting. Practically it would mean that there is no relation between the effects of the parameters between different groups, e.g. the size of the building could increase price somewhere and decrease it elsewhere.

some conclusion











