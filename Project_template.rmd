---
title: "Housing price prediction"
author: "Anonymous"
output: 
  pdf_document: 
    toc: yes
    toc_depth: 1
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Introduction

Online services such as zillow zestimates [1] provide accuarate information on how much houses sell for using gathered data, providing useful information for the realtor and the person selling the house. This notebook explores the possibilities on using stan to build regression models to predict housing prices on an zipcode level.     

[1] https://www.zillow.com/zestimate/


```{r,message=FALSE, warning=FALSE}
library(rstan)
options(mc.cores = 4)#parallel::detectCores())
library(ggplot2)
library(matrixStats)
library(dplyr)
library(GGally)
library(corrplot)
library(reshape2)
library(ElemStatLearn)
library(glmnet)
library(plotmo)
library(Metrics)
library(bayesplot)
library(loo)
set.seed(42)
```

# 2. Dataset description

For the prediction task we have chosen House Sales in King County, USA dataset [2], which provides data for the houses sold between May 2014 / May 2015 in the area in an regression friendly form.   
[2] https://www.kaggle.com/harlfoxem/housesalesprediction

```{r}
houseprice = read.csv("data/kc_house_data.csv", header = TRUE)

#suffle rows to guarantee no row depencies 
houseprice = houseprice[sample(nrow(houseprice)),]

#drop colinear columns sqft_living = sqft_above + sqft_basement
houseprice = subset(houseprice, select = -c(sqft_above, sqft_basement))

houseprice$log_price = log(houseprice$price)
datecol <- as.POSIXct(houseprice$date, format="%Y%m%dT%H%M%S")
houseprice$date_num = as.numeric(datecol)
unique_zips = unique(houseprice$zipcode)
houseprice$mutated_zipcode = match(houseprice$zipcode,  unique_zips)
head(houseprice)
```

```{r}
M <- cor(houseprice[-2], method="spearman")
corrplot(M, method = "circle")
```

As the used dataset contains multiple predictors with linear and non-linear depencies, we use lasso regression to perform variable selection on the dataset to find an smaller subset of predictor variables to use in our model. This is neccessary for the purposes of the notebook to speed up the calculations and to better guarantee convergence. 
!Notice that Lasso regression estimates are calculated suing an linear model so they might not be the best predictiors for an non-linear model. 

```{R}
houseprice_scaled <- mutate_if(houseprice, is.numeric, list(~scale(.) %>% as.vector))

response = houseprice_scaled[3]
obs = houseprice_scaled[4:19]
ridge_regression <- glmnet(y=data.matrix(response), x=data.matrix(obs), alpha = 1)
plot_glmnet(ridge_regression, xvar = "lambda", label = TRUE)
```

From the lasso regression plot we can see that: sqft_living, grade, lat, view, waterfront, yr_built, sqft_living15, bathrooms, long, and yr_renovated are the 10 best variables for the model.
When they are plotted in the matrix plot underneath, we can see that they chosen variables exhibit various linear and nonlinear effects on price.
We choose all of these variables for the stan model except for the waterfront variable. Waterfront variable is not used as its binary nature causes problems in convergence in the case of polynomial model used.

```{r,fig.height = 9, fig.width = 9}
ggpairs(houseprice  %>% select(price, sqft_living, grade, lat, view, waterfront, yr_built, 
                               sqft_living15, long, bathrooms))
```


# 3. Model description


## 3.1 Prior choices

In [3] it is recommended to scale the parameters to unit scale and to use student-t distribution $t_\nu(0, 1)$, where $3<\nu<7$, as a prior for linear regression coefficients. Student-t distribution has heavier tails than a normal distribution, but less heavy tails than a cauchy distribution, making it able to predict further away values while still keeping most of the mass near the mean.

$$
t_{\nu_{pdf}} = \frac{\Gamma\frac{\nu+1}{2}}{\sqrt{\nu\pi}\Gamma\frac{\nu}{2}}\bigg(1+\frac{x^2}{\nu}\bigg)^{-\frac{\nu+1}{2}}
$$

[3] https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations#prior-for-linear-regression

## 3.2 Stan models

We have built the model

[4] https://mc-stan.org/users/documentation/case-studies/radon.html

$$
y_i=\alpha_{j[i]}+\beta x_i+\epsilon_i
$$
where $j=1,\dots,70$ denotes the group of the observation. and $\epsilon_i \sim N(0, \sigma)$. The model can also be written as $y_i \sim N(\alpha_{j[i]}+\beta x_i, \sigma)$.


### Grouped multiple linear

```{r}
cat(readLines('models/grouped_multiple_linear.stan'), sep='\n')
```


$$
y_i=\alpha_{j[i]}+\beta x_i+\gamma x_i^2+\epsilon_i
$$
### Grouped multiple polynomial

```{r}
cat(readLines('models/grouped_multiple_polynomial.stan'), sep='\n')
```



## 3.3 Running the models

We train the models on 80% of the data and test the prediction accuracy on 20% of the dataset

```{r}
usable_numeric_columns = c("sqft_living", "grade","view", "lat", "yr_built", 
                           "sqft_living15","long", "bathrooms")
```


```{r}
training_indices = 0:16000
testing_indices = 16001:20000

used_columns = usable_numeric_columns
target_column = c("log_price")
group_column =  c("mutated_zipcode")
original_target = houseprice[,target_column]
training_data = houseprice_scaled[training_indices,used_columns]
testing_data =  houseprice_scaled[testing_indices, used_columns]
training_target = houseprice_scaled[training_indices,target_column]
testing_target_scaled =  houseprice_scaled[testing_indices, target_column]
testing_target =  houseprice[testing_indices, target_column]

X_var = training_data
X_var_pred = testing_data
y_var = training_target
group_var = houseprice[training_indices,group_column]
group_var_pred = houseprice[testing_indices,group_column]
  
data_list = list(
  X = X_var,
  X_pred = X_var_pred,
  K = ncol(X_var),
  N = nrow(X_var),
  N_pred = nrow(X_var_pred),
  N_groups = length(unique_zips),
  y = y_var,
  groups = group_var,
  groups_pred  = group_var_pred
)
head(X_var)
```



### Grouped multiple linear


```{r}
multiple_linear_fit <- stan(file = 'models/grouped_multiple_linear.stan', data = data_list)
```


```{r}
denormalize_results <- function(new_values, sd, mean){
  return (new_values * sd + mean)
}
orig_sd = sd(original_target)
orig_mean = mean(original_target)
```

```{r}
predicted_draws = extract(multiple_linear_fit)$y_pred
predicted_raws = colQuantiles(predicted_draws, probs = c(0.05, 0.5, 0.95))
predicted_prices = denormalize_results(predicted_raws, orig_sd, orig_mean)
```


```{r}
result_testing = data.frame(price = testing_target, predicted = predicted_prices)
ggpairs(result_testing, columns = c("price", "predicted.50."))
#, #ggplot2::aes(colour=factor(as.integer(lat*20)), alpha=0.75)) 
```


```{r}
mae_lin = mae(exp(testing_target),exp(predicted_prices))
mae_lin
```


 
```{r}
violin_predicted = extract(multiple_linear_fit)$y_pred
violin_predicted = exp(denormalize_results(violin_predicted, orig_sd, orig_mean))
#violin_predicted = exp(predicted_prices)
violin_groups = outer(1:nrow(violin_predicted), 1:ncol(violin_predicted),
                      FUN=function(r,c) unique_zips[group_var_pred[c]] )
violin_predicted = c(t(violin_predicted))
violin_groups = as.factor(c(t(violin_groups)))
violin_data_list_thing = data.frame(price=violin_predicted, group=violin_groups)

p <- ggplot(violin_data_list_thing, aes(x=group, y=price)) + 
  geom_violin()
p
```

### Grouped multiple polynomial

```{r}
X_var_second = X_var^2
X_var_pred_second = X_var_pred^2

#not used (third degree polynomial model data)
X_var_third = X_var^3
X_var_pred_third = X_var_pred^3

data_list = list(
  X = X_var,
  X_second = X_var_second,
  X_third = X_var_third,
  X_pred = X_var_pred,
  X_pred_second = X_var_pred_second,
  X_pred_third = X_var_pred_third,
  K = ncol(X_var),
  N = nrow(X_var),
  N_pred = nrow(X_var_pred),
  N_groups = length(unique_zips),
  y = y_var,
  groups = group_var,
  groups_pred  = group_var_pred
)
```



```{r}
multiple_polynomial_fit <- stan(file = 'models/grouped_multiple_polynomial.stan',
                                data = data_list)
```

```{r}
predicted_draws = extract(multiple_polynomial_fit)$y_pred
predicted_raws = colQuantiles(predicted_draws, probs = c(0.05, 0.5, 0.95))
predicted_prices = denormalize_results(predicted_raws, orig_sd, orig_mean)
```


```{r}
result_testing = data.frame(price = testing_target, predicted = predicted_prices)
ggpairs(result_testing, columns = c("price", "predicted.50."))
#, #ggplot2::aes(colour=factor(as.integer(lat*20)), alpha=0.75)) 
```


```{r}
mae_pol = mae(exp(testing_target),exp(predicted_prices))
mae_pol
```


# 4. Convergence diagnostics

From the plots of the models we can see that all model parameters have converged.

## Grouped multiple linear

```{r}
print(multiple_linear_fit, pars = c("alpha", "beta"))
```

```{R}
posterior_divergences <- as.array(multiple_linear_fit)
mcmc_trace(multiple_linear_fit, regex_pars = "beta")
```

## Grouped multiple polynomial


```{r}
print(multiple_polynomial_fit,  pars = c("alpha", "beta", "beta_second"))
```


```{R}
#posterior_divergences <- as.array(multiple_polynomial_fit)
#np = nuts_params(posterior_divergences)
mcmc_trace(multiple_polynomial_fit, regex_pars = "beta")
```







# 5. Posterior predictive checking

```{r}
replicated_data_lin = denormalize_results(extract(multiple_linear_fit)$y_rep, orig_sd, orig_mean)
replicated_data_pol = denormalize_results(extract(multiple_polynomial_fit)$y_rep, orig_sd, orig_mean)
#ppc_data = data.frame(original=original_target, linear=replicated_data_lin, polynomial=replicated_data_pol)

plot(density(replicated_data_lin), col="red")
lines(density(replicated_data_pol), col="blue")
lines(density(original_target))
#ggplot(ppc_data) +
#  geom_density()
#p1 = hist(replicated_data_pol, breaks = 50, )
#p2 = hist(original_target, breaks = 50)

#hist(exp(replicated_data), breaks = 50)
#hist(exp(original_target), breaks = 100)
```


```{r}
original_training_order = order(original_target[training_indices])
loo_lin <- loo(multiple_linear_fit, save_psis = TRUE)
psis_lin <- loo_lin$psis_object
lw_lin <- weights(psis_lin)
pp_check(c(original_target[training_indices]), yrep = replicated_data_lin, fun = "stat")
ppc_loo_pit_overlay(c(original_target[training_indices]), yrep = replicated_data_lin,
                    lw = lw_lin)
ppc_loo_ribbon(c(original_target[training_indices][original_training_order]), yrep = replicated_data_lin[,original_training_order],
               lw = lw_lin, psis_object = psis_lin)
ppc_loo_intervals(c(original_target[training_indices]),
                  yrep = replicated_data_lin, psis_object = psis_lin)
```

```{r}

loo_pol <- loo(multiple_polynomial_fit, save_psis = TRUE)
psis_pol <- loo_pol$psis_object
lw_pol <- weights(psis_pol)
pp_check(c(original_target[training_indices]), yrep = replicated_data_pol, fun = "stat")
ppc_loo_pit_overlay(c(original_target[training_indices]), yrep = replicated_data_pol,
                    lw = lw_pol)
ppc_loo_ribbon(c(original_target[training_indices][original_training_order]), yrep = replicated_data_pol[,original_training_order],
               lw = lw_pol, psis_object = psis_pol)
ppc_loo_intervals(c(original_target[training_indices]),
                  yrep = replicated_data_pol, psis_object = psis_pol)
```


# 6. Predictive performance assesment

From the mean squared errors we can see that the polynomial model performed better on the test set 
```{r}
# compare errors
data.frame(linear = mae_lin, polynomial = mae_pol)
```


## PSIS-lOO 

Obtained elpd information criteria values of the two models are largely the same with the polynomial model having an larger value, suggesting it is better of the two models. The k-values of the models are small suggesting the models fit the data wellÂ§.

### Multiple linear regression

```{r}
# Extract log-likelihood
multiple_linear_log_lik <- extract_log_lik(multiple_linear_fit, merge_chains = FALSE)

# PSIS-LOO elpd values
r_eff <- relative_eff(exp(multiple_linear_log_lik))
multiple_linear_loo_lin <- loo(multiple_linear_log_lik, r_eff = r_eff)

#elpd loo
multiple_linear_loo_lin
```


```{r}
pareto_k_table(multiple_linear_loo_lin)
```

```{r}
plot(multiple_linear_loo_lin, diagnostic = c("k", "n_eff"), label_points = FALSE,
  main = "PSIS diagnostic plot for ther multiple linear model")

```


### Multiple polynomial regression

```{r}
# Extract log-likelihood
multiple_polynomial_log_lik <- extract_log_lik(multiple_polynomial_fit, merge_chains = FALSE)

# PSIS-LOO elpd values
r_eff <- relative_eff(exp(multiple_polynomial_log_lik))
multiple_polynomial_loo_lin <- loo(multiple_polynomial_log_lik, r_eff = r_eff)

#elpd loo
multiple_polynomial_loo_lin
```

```{r}
pareto_k_table(multiple_polynomial_loo_lin)
```

```{r}
plot(multiple_polynomial_loo_lin, diagnostic = c("k", "n_eff"), label_points = FALSE,
  main = "PSIS diagnostic plot for ther multiple polynomial model")
```

## p_eff values

```{R}
#print(multiple_linear_loo_lin$p_loo)
#print(multiple_polynomial_loo_lin$p_loo)
#loo_compare(waic(multiple_linear_log_lik), waic(multiple_polynomial_log_lik))
loo_compare(x = list(multiple_linear_loo_lin, multiple_polynomial_loo_lin))
```

# 7. Discussion


Wrt using separate beta for each group:
There are 70 groups, so using a different beta value for each parameter for each group would increase the number of parameters of the model by 2-17 times, likely slowing the model. In addition, the number of data usable for each beta value would shrink. The dataset is large, so using most of the dataset for training, it shouldn't be a problem but with less data it could lead to overfitting. Practically it would mean that there is no relation between the effects of the parameters between different groups, e.g. the size of the building could increase price somewhere and decrease it elsewhere, which sounds counterintuitive, but could still be an avenue for future research.



























