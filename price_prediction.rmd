---
title: "House selling price prediction"
author: "Anonymous"
output: 
  pdf_document: 
    toc: yes
    toc_depth: 1
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dev="png")
knitr::opts_chunk$set(dpi=200)
```

# 1. Introduction

Online services such as zillow zestimates [1] provide accuarate information on how much houses sell for using gathered data, providing useful information for the realtor and the person selling the house. This notebook explores the possibilities on using stan to build regression models to predict housing prices on an zipcode level and explores how the different variables effect the price. 

For this purpose we conduct an small data exploration on an real world dataset. Fit two competing models an an linear and an nonlinear varying intercept model. Compare their performance on a test set and through model diagnostics. Finally we conclude with a discussion of the results.   

[1] https://www.zillow.com/zestimate/


```{r,message=FALSE, warning=FALSE}
library(rstan)
options(mc.cores = 4)#parallel::detectCores())
library(loo)
library(bayesplot)
library(ggplot2)
library(matrixStats)
library(dplyr)
library(GGally)
library(corrplot)
library(reshape2)
library(ElemStatLearn)
library(glmnet)
library(plotmo)
library(Metrics)
source('stan_utility.R')
set.seed(42)

```

# 2. Dataset description

For the prediction task we have chosen House Sales in King County, USA dataset [2], which provides data for the houses sold between May 2014 / May 2015 in the area in an regression friendly form. We transform the price dependant variable on a log-scale and the independent variables to 0 mean and 1 variance (mean center and unit variance scale) to guarantee better numerical accuracy and faster convergence. We also remove variables sqft_above and sqft_basement from the dataset as they are colinear wth sqft_living. 

[2] https://www.kaggle.com/harlfoxem/housesalesprediction

```{r}
houseprice = read.csv("data/kc_house_data.csv", header = TRUE)

#suffle rows to guarantee no row depencies 
houseprice = houseprice[sample(nrow(houseprice)),]

#drop colinear columns sqft_living = sqft_above + sqft_basement
houseprice = subset(houseprice, select = -c(sqft_above, sqft_basement))

#transform the depended variable to log scale to  ensure better numerical accuracy
houseprice$log_price = log(houseprice$price)


datecol <- as.POSIXct(houseprice$date, format="%Y%m%dT%H%M%S")
houseprice$date_num = as.numeric(datecol)
unique_zips = unique(houseprice$zipcode)
houseprice$mutated_zipcode = match(houseprice$zipcode,  unique_zips)

head(houseprice)
```

```{r}
M <- cor(houseprice[-2], method="spearman")
corrplot(M, method = "circle")
```
As the used dataset contains multiple predictors with linear and non-linear depencies, we use lasso regression to perform variable selection on the dataset to find an smaller subset of predictor variables to use in our model. This is neccessary for the purposes of the notebook to speed up the calculations and to better guarantee convergence. Also, the lasso regression plot beneath suggests that most of the variance in price can be explain by a much smaller subset of features.

!!! Notice that Lasso regression estimates are calculated using an linear model so they might not be the best predictors for an non-linear model. 

```{r}
hist(houseprice$log_price, breaks=50, main="Histogram of logarithm of house prices")
```


```{R}
houseprice_scaled <- mutate_if(houseprice, is.numeric, list(~scale(.) %>% as.vector))

response = houseprice_scaled[3]
obs = houseprice_scaled[4:19]
ridge_regression <- glmnet(y=data.matrix(response), x=data.matrix(obs), alpha = 1)
plot_glmnet(ridge_regression, xvar = "lambda", label = TRUE)
```

From the lasso regression plot we can see that: sqft_living, grade, lat, view, waterfront and yr_built are the 6 best variables for the model.
When they are plotted in the matrix plot underneath, we can see that they chosen variables exhibit various linear and nonlinear effects on price.

We choose 5 of these variables for the stan model except for the waterfront variable as its binary nature causes problems in convergence in the case of polynomial model used.

```{r,fig.height = 9, fig.width = 9}
ggpairs(houseprice  %>% select(price, sqft_living, grade, lat, view,waterfront,yr_built))
```


# 3. Model description

We fit two varying intercept regression models: an multiple linear and an multiple polynomial model. Varying intercept models [3] are multilevel models, which let the intercept parameter vary between the groups while sharing the slope parameters. This is useful as the different zipcodes likely have different price levels that we can take into account in the intercept. We also could consider varying slope parameters by the category, but getting the model to convergece with this little data would likely be difficult.  


[3] https://psmits.github.io/paleo_book/varying-intercept-models.html

## 3.1 Prior choices

In [4] it is recommended to scale the parameters to unit scale and to use student-t distribution $t_\nu(0, 1)$, where $3<\nu<7$, as a prior for linear regression coefficients. Student-t distribution has heavier tails than a normal distribution, but less heavy tails than a cauchy distribution, making it able to predict further away values while still keeping most of the mass near the mean.

$$
t_{\nu_{pdf}} = \frac{\Gamma\frac{\nu+1}{2}}{\sqrt{\nu\pi}\Gamma\frac{\nu}{2}}\bigg(1+\frac{x^2}{\nu}\bigg)^{-\frac{\nu+1}{2}}
$$

[4] https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations#prior-for-linear-regression

## 3.2 Stan models

We have built the models using stan radon case study [6] as a starting point to build our regression models. We have expanded on the varying intercept model of the example by adding multiple linear and polynomial terms into the model. In our model intercept parameters vary by the zipcode while the slope parameters are shared across zipcodes.

[5] https://mc-stan.org/users/documentation/case-studies/radon.html


### Grouped multiple linear

$$
y_i=\alpha_{j[i]}+\beta x_i+\epsilon_i
$$
where $j=1,\dots,70$ denotes the group of the observation. and $\epsilon_i \sim N(0, \sigma)$. The model can also be written as $y_i \sim N(\alpha_{j[i]}+\beta x_i, \sigma)$.

```{r}
cat(readLines('models/grouped_multiple_linear.stan'), sep='\n')
```

### Grouped multiple polynomial

$$
y_i=\alpha_{j[i]}+\beta x_i+\gamma x_i^2+\epsilon_i
$$

```{r}
cat(readLines('models/grouped_multiple_polynomial.stan'), sep='\n')
```



## 3.3 Running the models

We train the models on 80%/20%-test split using 10000 first datapoints. Models are fitted for all the 71 zipcodes. Full data is not used as loo fails with large datasets.

```{r}
usable_numeric_columns = c("sqft_living", "grade","view", "lat", "yr_built")
```


```{r}
training_indices = 0:8000
testing_indices = 8001:10000

used_columns = usable_numeric_columns
target_column = c("log_price")
group_column =  c("mutated_zipcode")
original_target = houseprice[,target_column]
training_data = houseprice_scaled[training_indices,used_columns]
testing_data =  houseprice_scaled[testing_indices, used_columns]
training_target = houseprice_scaled[training_indices,target_column]
testing_target_scaled =  houseprice_scaled[testing_indices, target_column]
testing_target =  houseprice[testing_indices, target_column]

X_var = training_data
X_var_pred = testing_data
y_var = training_target
group_var = houseprice[training_indices,group_column]
group_var_pred = houseprice[testing_indices,group_column]
  
data_list = list(
  X = X_var,
  X_pred = X_var_pred,
  K = ncol(X_var),
  N = nrow(X_var),
  N_pred = nrow(X_var_pred),
  N_groups = length(unique_zips),
  y = y_var,
  groups = group_var,
  groups_pred  = group_var_pred
)
head(X_var)
```

```{r}
denormalize_results <- function(new_values, sd, mean){
  return (new_values * sd + mean)
}
orig_sd = sd(original_target)
orig_mean = mean(original_target)
```


### Grouped multiple linear


```{r}
multiple_linear_fit <- stan(file = 'models/grouped_multiple_linear.stan', data = data_list)
```

```{r}
predicted_draws = extract(multiple_linear_fit)$y_pred
predicted_raws = colQuantiles(predicted_draws, probs = c(0.05, 0.5, 0.95))
predicted_prices = denormalize_results(predicted_raws, orig_sd, orig_mean)
rep_prices = extract(multiple_linear_fit)$y_rep %>% colQuantiles(probs = c(0.05, 0.5, 0.95)) %>% denormalize_results(orig_sd, orig_mean)
```

```{r}
result_testing = data.frame(price = (testing_target), predicted = (predicted_prices))
ggpairs(result_testing, columns = c("price", "predicted.50."))
```

```{r}
#result_rep = data.frame(price = exp(original_target[training_indices]), predicted = exp(rep_prices))
#ggpairs(result_rep, columns = c("price", "predicted.50."))
#mae(exp(original_target[training_indices]),exp(rep_prices))
#rep_order = order(original_target[training_indices])
#rep_error = abs(exp(original_target[training_indices]) - exp(rep_prices))
#plot(original_target[training_indices][testing_rising_order], rep_error[testing_rising_order], col="#00000018", ylab = "error", xlab = "log price", main = "Error per log(price)")
```

We can see that the best parameters correspond to lasso regression best parameters. Fourth parameter corresponding to view quality has a very wide posterior distribution. 

```{r}
posterior_linear <- as.matrix(multiple_linear_fit)
posterior_linear <- posterior_linear[,c("beta[1]", "beta[2]","beta[3]","beta[4]","beta[5]")]


plot_title <- ggtitle("Posterior distributions multiple polynomial",
                      "with medians and 80% intervals")

mcmc_areas(posterior_linear,
           pars = c("beta[1]", "beta[2]","beta[3]","beta[4]","beta[5]"),
           prob = 0.8) + plot_title
```

Mean absolute error is quite large

```{r}
mae_lin = mae(exp(testing_target),exp(predicted_prices))
mae_lin
```

The error increases as a function of price

```{r}
testing_rising_order = order(testing_target)
error = abs(exp(testing_target) - exp(predicted_prices))
plot(testing_target[testing_rising_order], error[testing_rising_order], col="#00000018", ylab = "error", xlab = "log price", main = "Error per log(price)")
lines(c(0, 20000), c(150000, 150000), col = "#ff000080")
#hist(error, breaks = 500, xlim = c(0, 2e6))
#lines(c(150000, 150000), c(0, 20000), col = "red")
```

```{r}
#TODO: Redo the zipcode plot
violin_predicted = extract(multiple_linear_fit)$y_pred
violin_predicted = exp(denormalize_results(violin_predicted, orig_sd, orig_mean))
#violin_predicted = exp(predicted_prices)
violin_groups = outer(1:nrow(violin_predicted), 1:ncol(violin_predicted),
                      FUN=function(r,c) unique_zips[group_var_pred[c]] )
violin_predicted = c(t(violin_predicted))
violin_groups = as.factor(c(t(violin_groups)))
violin_data_list_thing = data.frame(price=violin_predicted, group=violin_groups)

p <- ggplot(violin_data_list_thing, aes(x=group, y=price)) + 
  geom_violin()
p
```

### Grouped multiple polynomial

```{r}
X_var_second = X_var^2
X_var_pred_second = X_var_pred^2

#not used (third degree polynomial model data)
X_var_third = X_var^3
X_var_pred_third = X_var_pred^3

data_list = list(
  X = X_var,
  X_second = X_var_second,
  X_third = X_var_third,
  X_pred = X_var_pred,
  X_pred_second = X_var_pred_second,
  X_pred_third = X_var_pred_third,
  K = ncol(X_var),
  N = nrow(X_var),
  N_pred = nrow(X_var_pred),
  N_groups = length(unique_zips),
  y = y_var,
  groups = group_var,
  groups_pred  = group_var_pred
)
```


```{r}
multiple_polynomial_fit <- stan(file = 'models/grouped_multiple_polynomial.stan',
                                data = data_list)
```

```{r}
predicted_draws = extract(multiple_polynomial_fit)$y_pred
predicted_raws = colQuantiles(predicted_draws, probs = c(0.05, 0.5, 0.95))
predicted_prices = denormalize_results(predicted_raws, orig_sd, orig_mean)
```


```{r}
result_testing = data.frame(price = testing_target, predicted = predicted_prices)
ggpairs(result_testing, columns = c("price", "predicted.50."))
```



From the first order polynomial part the parameters, which had the most effect correspond to lasso regression best parameters. 

```{r}
posterior_polynomial <- as.matrix(multiple_polynomial_fit)
posterior_polynomial <- posterior_polynomial[,c("beta[1]", "beta[2]","beta[3]","beta[4]","beta[5]",
                                  "beta_second[1]", "beta_second[2]","beta_second[3]","beta_second[4]","beta_second[5]")]


plot_title <- ggtitle("Posterior distributions multiple polynomial",
                      "with medians and 80% intervals")

mcmc_areas(posterior_polynomial,
           pars = c("beta[1]", "beta[2]","beta[3]","beta[4]","beta[5]"),
           prob = 0.8) + plot_title
```


```{r}
plot_title <- ggtitle("Posterior distributions multiple polynomial 2 degree slope",
                      "with medians and 80% intervals")

mcmc_areas(posterior_polynomial,
           pars = c("beta_second[1]", "beta_second[2]","beta_second[3]","beta_second[4]","beta_second[5]"),
           prob = 0.8) + plot_title
```

Mean absolute error still is quite large, but there is a small improved compared to linear model.

```{r}
mae_pol = mae(exp(testing_target),exp(predicted_prices))
mae_pol
```

```{r}
testing_rising_order = order(testing_target)
error = abs(exp(testing_target) - exp(predicted_prices))
plot(testing_target[testing_rising_order], error[testing_rising_order], col="#00000018", ylab = "error", xlab = "log price", main = "Error per log(price)")
lines(c(0, 20000), c(150000, 150000), col = "#ff000080")
#hist(error, breaks = 500, xlim = c(0, 2e6))
#lines(c(150000, 150000), c(0, 20000), col = "red")
```


# 4. Convergence diagnostics

## Grouped multiple linear

From the model diagnostics we can see that that the model has converged.

```{r}
check_all_diagnostics(multiple_linear_fit)
```

```{R}
posterior_divergences <- as.array(multiple_linear_fit)
mcmc_trace(multiple_linear_fit, regex_pars = "beta")
```

## Grouped multiple polynomial

From the model diagnostics we can see that that the model has converged.

```{r}
#print(multiple_polynomial_fit,  pars = c("alpha", "beta", "beta_second"))
check_all_diagnostics(multiple_linear_fit)
```

```{r}
mcmc_trace(multiple_polynomial_fit, regex_pars = "beta")
```

# 5. Posterior predictive checking

From the plot beneath, we can see that replicated data is nearly indistinguishable from the target

```{r}
replicated_data_lin = denormalize_results(extract(multiple_linear_fit)$y_rep, orig_sd, orig_mean)
replicated_data_pol = denormalize_results(extract(multiple_polynomial_fit)$y_rep, orig_sd, orig_mean)

plot(density(replicated_data_lin), col="red", main="Replicated posterior" )
lines(density(replicated_data_pol), col="blue")
lines(density(original_target), col="black")
legend(x="topright",
       legend=c("Linear replicated", "Polynomial Replicated", "Original data"),
       col=c("red", "blue", "black"), lty=1:1, cex=0.8)
```

## Multiple linear model Leave-One-Out (LOO) predictive checks

Loo pit plot suggests that our model is not able to predict all the variance in the data.

```{r}
original_training_order = order(original_target[training_indices])
loo_lin <- loo(multiple_linear_fit, save_psis = TRUE,cores = getOption("mc.cores", 4))
psis_lin <- loo_lin$psis_object
lw_lin <- weights(psis_lin)
pp_check(c(original_target[training_indices]), yrep = replicated_data_lin, fun = "stat")
ppc_loo_pit_overlay(c(original_target[training_indices]), yrep = replicated_data_lin,
                    lw = lw_lin)
ppc_loo_ribbon(c(original_target[training_indices][original_training_order]),
               yrep = replicated_data_lin[,original_training_order],
               lw = lw_lin, psis_object = psis_lin)
ppc_loo_intervals(c(original_target[training_indices]),
                  yrep = replicated_data_lin, psis_object = psis_lin)
```


## Multiple polynomial model Leave-One-Out (LOO) predictive checks

Loo pit plot suggests that our model is not able to predict all the variance in the data.

```{r}

loo_pol <- loo(multiple_polynomial_fit, save_psis = TRUE)
psis_pol <- loo_pol$psis_object
lw_pol <- weights(psis_pol)
pp_check(c(original_target[training_indices]), yrep = replicated_data_pol, fun = "stat")
ppc_loo_pit_overlay(c(original_target[training_indices]), yrep = replicated_data_pol,
                    lw = lw_pol)
ppc_loo_ribbon(c(original_target[training_indices][original_training_order]),
               yrep = replicated_data_pol[,original_training_order],
               lw = lw_pol, psis_object = psis_pol)
ppc_loo_intervals(c(original_target[training_indices]),
                  yrep = replicated_data_pol, psis_object = psis_pol)
```


# 6. Predictive performance assesment

From the mean squared errors we can see that the polynomial model performed better on the test set

```{r}
# compare errors
data.frame(linear = mae_lin, polynomial = mae_pol)
```

## PSIS-lOO 

Obtained elpd information criteria values of the two models are largely the same with the polynomial model having an larger value, suggesting it is better of the two models. The k-values of the models are small expect for one observations for both models suggesting the models fit the data well. The bad k-value is likely caused by the number of observations being too small as I was not able to run the model with the whole dataset. Still the model might be misspesified as p_loo < number of parameters, which corresponds to model being too flexible or having too weak of a population prior [6].       

[6] https://mc-stan.org/loo/reference/loo-glossary.html

### Multiple linear regression

```{r}
# Extract log-likelihood
multiple_linear_log_lik <- extract_log_lik(multiple_linear_fit, merge_chains = FALSE)

# PSIS-LOO elpd values
r_eff <- relative_eff(exp(multiple_linear_log_lik))
multiple_linear_loo_lin <- loo(multiple_linear_log_lik, r_eff = r_eff)

#elpd loo
multiple_linear_loo_lin
```


```{r}
pareto_k_table(multiple_linear_loo_lin)
```

```{r}
plot(multiple_linear_loo_lin, diagnostic = c("k", "n_eff"), label_points = FALSE,
  main = "PSIS diagnostic plot for ther multiple linear model")
```


### Multiple polynomial regression

```{r}
# Extract log-likelihood
multiple_polynomial_log_lik <- extract_log_lik(multiple_polynomial_fit, merge_chains = FALSE)

# PSIS-LOO elpd values
r_eff <- relative_eff(exp(multiple_polynomial_log_lik))
multiple_polynomial_loo_lin <- loo(multiple_polynomial_log_lik, r_eff = r_eff)

#elpd loo
multiple_polynomial_loo_lin
```

```{r}
pareto_k_table(multiple_polynomial_loo_lin)
```

```{r}
plot(multiple_polynomial_loo_lin, diagnostic = c("k", "n_eff"), label_points = FALSE,
  main = "PSIS diagnostic plot for ther multiple polynomial model")
```

## elpd_loo comparison

Polynomial Model is sligtly better according to elped information criteria.

```{R}
loo_compare(x = list(multiple_linear_loo_lin, multiple_polynomial_loo_lin))
```

# 7. Discussion

In this report we have explored linear and polynomial regression models for predicting house prices. The differences between the results from the models are small, but the polynomial model performs a bit better. The mean absolute error for both models is over hundred thousand, but considering the mean of the prices is around five hundred thousand, the error rate is small considering the simplicity of the model.

In the future we could consider varying slope parameter by zipcode. This would mean the different predictors would have different effects in the model depending on the zipcode they belong to, which would help us to better undestand how different factors effect housing prices in geograhical areas. However this has few techical drawbacks. There are 70 groups, so using a different beta value for each parameter for each group would increase the number of parameters of the model considerably, likely slowing the model. In addition, the number of data usable for each beta value would shrink, which likely would lead to problems in convergence and biased estimates. 





